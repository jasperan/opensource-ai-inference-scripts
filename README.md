# opensource-ai-inference-scripts
My scripts to achieve opensource (ollama, vLLM and llama.cpp) inference on local commodity GPUs
